Disputatio. Philosophical Research Bulletin Vol. 7 | No. 8 | Dec. 2018 | a010 | ISSN: 2254-0601 | www.disputatio.eu  © The author(s) 2018. This work, published by Disputatio [www.disputatio.eu], is an Open Access article distributed under the terms of the Creative Commons License [BY–NC–ND]. The copy, distribution and public communication of this work will be according to the copyright notice (https://disputatio.eu/info/copyright/). For inquiries and permissions, please email: (✉) boletin@disputatio.eu.  
 
Practices of Governing and Making Artificial Intelligences   José Miguel Samaniego Technical University of Munich, Germany e-mail: samaniegoeguiguren@gmail.com   ABSTRACT With an emphasis on Science and Technology Studies (STS), this essay inquires AI standardization (AIS). Through a social cartography and its underlying conceptual tools, it concludes that AI standards are socio–technical hybrids across several domains, made out of associations of heterogeneous entities, which address multiple artificial intelligences. AI standards bear high stakes because they are more than just technical matters: they are political, they take a lot of socio–technical and infrastructural work, and they contribute to how AI is practiced. In AIS webs of relations, AI and related issues are ontologically multiple, which rises important questions about the politics and ethics of AI.  WORK TYPE  Article  ARTICLE HISTORY  Received: 20–June–2018 Accepted:  22–December–2018  ARTICLE LANGUAGE  English  KEYWORDS AI standards STS ANT Social Cartography  Ontological Multiplicity © Studia Humanitatis – Universidad de Salamanca 2018      NOTES ON CONTRIBUTOR José Miguel Samaniego holds a major in Electrical/Mechanical Engineering from Universidad Nacional de Loja. MPhil at the Technical University of Munich (TUM) in the Science and Technology Studies (STS) program. He has worked as a research assistant at the Reorganizing Industries Post/Doc Lab from the Munich Center for Technology in Society (MCTS), and as an intern at the Bavarian Broadcast (BR) organization.   HOW TO CITE THIS ARTICLE Samaniego, José Miguel (2018). «Practices of Governing and Making Artificial Intelligences». Disputatio. Philosophical Research Bulletin 7, no. 8: a010. 

Disputatio. Philosophical Research Bulletin Vol. 7 | No. 8 | Dec. 2018 | a010 | ISSN: 2254-0601 | www.disputatio.eu  © The author(s) 2018. This work, published by Disputatio [www.disputatio.eu], is an Open Access article distributed under the terms of the Creative Commons License [BY–NC–ND]. The copy, distribution and public communication of this work will be according to the copyright notice (https://disputatio.eu/info/copyright/). For inquiries and permissions, please email: (✉) boletin@disputatio.eu.  
 
Practices of Governing and Making Artificial Intelligences  José Miguel Samaniego    §1. Introduction RTIFICIAL INTELLIGENCE (AI) captures imaginations in a spectrum that goes from gloom dystopias to content utopias. Academic accounts, media portrayals, industry promises, political talk; they range from the sober and attentive of actual practices, to the hyped up and futuristic. While browsing AI literatures for this work, I saw many images of imagined white–head androids with stoic expression gazing at something away from the eyes of the beholder; I saw images of artificial neurons networked within the contours of a human brain; I saw caricatures of what are supposed to be evil intelligent robots, good intelligent robots, indifferent intelligent robots; I saw confident automaton hands reaching for their human counterpart; I saw zeros and ones lining up with blue shades and cryptic digital aesthetics to signify revolutions of all kinds. Imaginations and representations abound, and they do not go without implications for AI fields and for those that are influenced by them. For instance: the metaphors of ‘big data’ exaggerate its epistemological value and have consequences, such as, discrimination, symbolic violence, or large institutions benefiting from such metaphors (Couldry 2017); technological myths of machines that replicate the human mind attract funding and research, advance transhumanist and singularity ideologies, and shape contemporary discourse of digital technology and culture (Natale and Ballatore 2017); recurring imaginary futures of AI keep standing as promises of universal computing and, in a way, hide the impact of computing in several domains (Barbrook 2007); etcetera. AI future oriented imaginations, and unrealistic expectations of benefits and risks are not unique; in the meeting spaces of technoscience, markets and industries, hyperbolic claims and promises are quite common, and many times they are even instrumental to the creation of value and interest (Brown 2003). Trying not to succumb to hyperbolic hopes and fears, extreme promises or cynicism, phobias and dogmas; trying to add to the works that demystify AI, but that still assume that it is indeed a timely issue; trying to highlight the hard and mundane work that lurks behind the fancy talk; I turn to the study of current AI standardization. Susan Leigh Star opens her article, The Ethnography of Infrastructure, with a call to study “boring things”; for studying infrastructures —which are often made invisible— is investigating large technical systems “in the making” (Star 1999). Standards are part of infrastructures and are infrastructures in themselves, and they have long been observed to be anything but neutral: standardization is politically charged because some values and practices are adopted and supported over others (Stola and Bowker 2017). Moreover, with how ubiquitous standardization currently is, standards play a role in regulating and calibrating life by making or maintaining equivalences across times, spaces, and cultures (Timmermans and A	
PRACTICES OF GOVERNING AND MAKING ARTIFICIAL INTELLIGENCES  |  2   
Disputatio (2018) 8: a010  Epstein 2010). Most notably, current efforts of AI standardization focus on the creation of standards; about the ‘creation phase’, Timmermans and Epstein state the following:  [S]tandard creation is fundamentally a social act. Although theoretically one person could create a standard, most standards are built collectively and, in order to work in a standardized way, require some form of buy–in by multiple others. A key issue in studying standard creation is then to map the interactions among the multiple parties involved in the creation process, even paying attention to those that could reasonably be expected to be included but are currently not part of the creation process (2010, p. 75).  At the moment, AI standardization (AIS) is open to debate across many sites, and group formations span profusely along with their standards to govern and shape AI. Through a set of maps regarding AIS, this essay1 inquires the process of standardization and its related issues; in other words, themes around the creation of AI standards in recent years, the matters of standardization of different group formations, the entities and actors that inhabit AIS, the logics of standardization, the domains that AI standards touch upon, the heterogeneous relationality of standardization, and the ontological query of what is being standardized. In what follows, a summary of the conceptual tools behind the craft of the maps is given. In their book Issue Mapping for an Ageing Europe, Rogers, Sanchez and Kil open up with a tidy overview of approaches to mapping issues. For them, the three most influential practices of issue mapping are furthered by social cartography, risk cartography and neo–cartography. It is precisely social cartography that is of interest in this essay. Social cartography is a set of concepts and practices to map issues and controversies, developed by sociologists of associations and practitioners of actor–network theory (ANT) 2; its objective is to deploy the state of affairs of an issue (Rogers et al. 2015, p. 14). A subset of social cartography —relevant for this work— is controversy mapping3, developed by Bruno Latour and others as a didactic approach to sociology of associations and explained by Tommaso Venturini (2010a; 2010b) through a set of recommendations. A quick word must be addressed before tackling social cartography, namely, the concept of ‘issuefication’. Issuefication points to the intensification of the interest and activity around an object, effectively transforming it into an issue; thus, issuefication is becoming an issue (Rogers et al. 2015, p. 141). In this context, issuefication invites “actors to make a difference, and so to remain active and associated in a network of the issue” (Rogers et al. 2015, p. 41). Other vocabulary that describes similar dynamics, indeed that inspired the above mentioned  1  This essay is based on a master’s thesis I wrote at the Munich Center for Technology in Society, with direction from Dr. phil. Jan–Hendrik Passoth. 2  Rogers, Sanchez and Kill take Latour’s book Reassembling the Social (2005) as their guide to social cartography. In it, Latour deploys ideas from ANT to propose a ‘sociology of associations’. 3  Controversy mapping is used to describe and represent controversies specifically. For the purpose of this work, which deals with issues and controversies, ‘controversy mapping’ and ‘social cartography’ can be taken as interchangeable terms, as the former is a subset of the latter. 
3  |  JOSÉ MIGUEL SAMANIEGO   
 Disputatio (2018) 8: a010  concept, is the envisioned transition from ‘matters of fact’ to ‘matters of concern’ (MoC) by Latour; also known as a transition from objects to things. A historical example to understand the difference between these matters goes as follows. Once, the Challenger space shuttle was treated as a matter of fact or an object: “completely mastered, perfectly understood, quite forgotten by the media, taken–for–granted, matter–of–factual projectile” (Latour 2004, p. 234). When the shuttle disintegrated after lunch in 1986, thousands of people gathered around this new thing to collect clues for judicial scientific investigations and to open up all sorts of related debates; “in a stroke, an object had become a thing, a matter of fact was considered as a matter of great concern” (Latour 2004, p. 235). Issuefication and matters of concern hold especial relevance for social cartography and controversy mapping; they provide vocabulary regarding collective life and controversies, and are constant reminders of the associations that ensemble things. A guide to a social cartography would be better represented, according to Rogers et al. (2015), by the three following arguments from the sociology of associations. First, the redefinition of the social not as a pre–existing structure or special domain, but as a movement of actors in processes of assembling and reassembling. In social cartography, the “shift from structure to movement is a key insight [that] forces the analyst to trace instead of dig, expose or unveil” (Rogers et al. 2015, p. 15). In this context, the ‘social’ is what ought to be explained, not the explanatory category: “the social is not the explanation for the state of affairs of an issue; instead the state of affairs of an issue is precisely the social being performed by the actors” (Rogers et al. 2015, p. 15). Secondly, the redefinition of sociology as the practice of tracing associations. The social cartographer must map the “actions and associations that assemble different actors together into a state of affairs that is not pre–given but instead performative [...] It is the role of the researcher to trace these associations in order to describe how the social comes into being” (Rogers et al. 2015, pp. 15–16). Thirdly, the idea of distributed agency and the inclusion of non–humans as agents of a given network. Here, the researcher’s task is to map out agencies, being attentive to transformation of message or meaning and the format of action; social cartography requires taking seriously “how issues are made into matters of concern” (Rogers et al. 2015, p. 17). Regarding the inclusion of non–humans in a network of actors, the rule is simple, anything that “changes the state of affairs (that acts) is on the map (as a mediator)” (Rogers et al. 2015, p. 17). The authors attempt to bring together these themes with the statement: “ultimately, a good account traces the network and helps us to describe the state of affairs composed of actors and things that make other actors and things do something” (Rogers et al. 2015, p. 17). A cartographic approach specifically tuned to controversies is explored in Venturini’s consecutive papers Diving on Magma (2010b) and Building on Faults (2010a); here, I shortly paraphrase the implications of his recommendations. First, the actors’ take on an issue or debate is to be taken into account with priority in the act of mapping. Actors within the particular network are the experts, therefore, trying to impose the assumptions of the researcher over the myriad of stands in a controversy would be arrogant and miss the point of tracing actor’s associations. Second, in observing controversies, researchers must multiply 
PRACTICES OF GOVERNING AND MAKING ARTIFICIAL INTELLIGENCES  |  4   
Disputatio (2018) 8: a010  their viewpoints accordingly with the different perspectives in the controversy. Third, researchers should employ every method and tool at hand that would offer new perspectives of the controversy. Here, it is not considered ‘unbiased’ to commit to just one method or theory; on the contrary, one should be ‘promiscuous’ in the observations and descriptions of controversies. Fourth, the researcher must go back and forth between observation and description in the process of mapping, adjusting both at every turn. In this iterative process, ‘unfolding the complexity of controversies’ and ‘ordering’ them happens simultaneously since the beginning of the cartographic exercise, and the researcher improves both little by little. Fifth, to make controversies legible one should simplify their complexity for representation; however, this simplification is still expected to show the range of viewpoints around issues. Sixth, even if one is expected to deploy a full arrange of viewpoints, still these descriptions and representations must be proportional to the relevance of the actors for the issue at hand. Seventh, effective controversy mapping will: adapt to actors’ native representations of the issue, deploy different maps to account for each aspect of the controversy, and allow the audience of the map to retrieve a glance into the complexity of the controversy after the researcher simplifies it. The different layers of a controversy that one could focus on, and the recommended order to do so, is also pointed out by Venturini (2010b). According to him, focusing on each layer functions as different lenses for the researcher; then, like with a microscope, the lenses to be used are up to the researcher and their specific case. One could focus on statements, literatures, actors, networks, and cosmoses (2010b, pp. 265–267). A brief account of these layers will be stated now, following the approach furthered in the citation above. A first layer, and also a good place to start the observation of a controversy, is the plethora of statements regarding the controversial issue, i.e., the claims from actors involved. Often, these claims elicit reactions and new statements from other actors, which in turn make it possible to descry the arena of the controversy. As various statements are brought together and related to each other, one begins to see the first instantiations of what Venturini calls literatures. The task is to move from statements to literatures, that is, to map the “web of references, revealing how dispersed discourses are woven into articulated literatures” (2010b, p. 266). The networks in which the statements are found are comprised by many other entities as well, such as technologies and organisms, humans and non–humans, objects and things. These entities, as long as they act in the network, are the actors of the controversy at hand. Therefore, their position, agencies and relations, offer a new layer of focus in the controversy. Then, the lens of networks is powered by the ANT reminder that “there is no such thing as an isolated actor. Actors are always composed by and components of networks” (2010b, p. 267). Thus, to observe networks is to observe the incessant work of connections between actors that comprise the network or dissolve it, in other words, the dynamics of actor–networks. Finally, moving from networks to ideologies one would be applying the cosmoses lens. This layer of focus highlights the relevant ideologies in the network to enable a map of ‘cosmoses’. Ideologies affect collectives inasmuch as they influence decisions from actors.   
5  |  JOSÉ MIGUEL SAMANIEGO   
 Disputatio (2018) 8: a010  §2. An atlas of AI standardization What is the current state of affairs of artificial intelligence standardization (AIS)? How the standardization of artificial intelligence (AI) is being unfolded? How the AIS issue is rendered as a matter of concern? Answering these interrelated questions requires an overview of global AIS activities. In the context of AIS, the term ‘activities’ can be referred also as ‘debate’ or ‘controversy’, for it is currently a field of great movement, disagreements, rapid group formations, and heterogeneous actors involved in a network expecting to shape and govern AI through standards. Collaboration and rivalry come in different forms: from initiatives and enterprises, through (dis)similar AIS goals and national strategies, to heterogeneous understandings of the very object being standardized. Hence, an account for the posed questions is to be realized through an overview of the AIS controversial arena, with which: a state of affairs can be described —its complexities, issues, actors and networks—, the unfolding of AIS can be devised —its group formations, associations and issue spaces—, and the rendering of matters of concern (MoC) —that partly drives the AIS urge— can be highlighted. For better readability, this atlas is divided in sections that correspond with each ‘controversy layer’; nevertheless, these layers should not be thought as independent from each other. The process of observation–representation follows the movement from one layer to the next, and representation of a new layer is only possible through the insights gained with its predecessor.   §3. AIS layer of statements A first approximation to descry the AIS controversial arena is the collection of statements regarding AIS through a search engine resonance (Rogers et al. 2015). The selection process and reduction of the plethora of statements that respond to queries about ‘standardization’ follows the criteria of what is meant by ‘standardization’ and ‘standards’ in this essay. Stefan Timmermans and Steven Epstein (2010) define standardization as a “process of constructing uniformities across time and space, through the generation of agreed–upon rules”, and remind us that standards “tend to span more than one community of practice or activity site; they make things work together over distance or heterogeneous metrics; and they are usually backed up by external bodies of some sort, such as professional organizations, manufacturers’ associations, or the state” (p. 71). These definitions of ‘standards’ and ‘standardization’ guide the selection of entities included in this account —and the ones that were left out—. For instance, many statements retrieved through the resonance, but not portrayed, do not refer to standards in a parallel way to the mentioned definition; rather, they use the word to describe the ubiquity of certain technologies in some fields of industry or research. This use of the word ‘standard’ simply designates things that are common in a certain domain, instead of the institutionalized uniformities —technical, legal, ethical, etc.— that seek to ‘make things work together’; in this essay, only the latter is accounted for. The exploration of statements about AIS starts with a timeline (see Fig. 1.1 to 1.4) that goes back to the first statements that could be retrieved and up to 2018. 
PRACTICES OF GOVERNING AND MAKING ARTIFICIAL INTELLIGENCES  |  6   
Disputatio (2018) 8: a010   Figure 1.1. Chronology of dispute (1980–2015). It depicts a timeline of statements made about AIS; this type of representation shows how claims may elicit responses. Green text bubbles indicate the category of a statement followed by the author or organization. Orange text bubbles are the statements themselves, which are rephrased in a single sentence —to fit in the graph— that captures the original intention. The timeline was generated with the tools of the website time.graphics and adapted with GIMP.   
 Figure 1.2. Chronology of dispute (2016).      

7  |  JOSÉ MIGUEL SAMANIEGO   
 Disputatio (2018) 8: a010   
 Figure 1.3. Chronology of dispute (2017).   
 
 Figure 1.4. Chronology of dispute (2018). 

PRACTICES OF GOVERNING AND MAKING ARTIFICIAL INTELLIGENCES  |  8   
Disputatio (2018) 8: a010  The statements of the presented timelines are part of a broader set of statements that comprise articles of all sorts, still all of them addressing heterogeneous AIS matters. If we would take into account the whole set of statements and boil them down to their topics or keywords, then we would be faced with statement types. Figure 2 is an attempt to reduce all those voices to categories that map the themes often going along the AIS debate. Hence, the AIS controversial arena is pervaded by the following motifs: applications and fields related to AI systems and technologies; politic debates and political action around AI; technical reasons that demand or influence the standardization of AI; political and economic reasons to standardize AI; AIS issues and challenges; various concerns related to the implications of AI; ethical reasons that call for AIS; matters of AI policy, law and regulation; ontology and study of AI.  
 Figure 2. Types of statements identified. Statements are claims regarding AIS; the boxes of the pie chart contain keywords of those statements. The boxes encompass keywords and are color–coded in accordance to categories; the name of each category is next to each cross symbol. Because the boxes only contain a sample of the statements, the total number of keywords identified for each category is displayed on top of the corresponding box. The pie chart was generated in chartblocks.com and modified with GIMP.  If we were to collect the sources from which the statements and keywords were harvested, and make a resonance out of each source to quantify the appearances of every keyword, and furthermore, aggregate all the keyword appearances in a single representation, we would obtain a hierarchical map of the most prominent issues in the AIS debate. That is precisely what Figure 3 does by ordering the issues of the statements layer by size. One could look at 

9  |  JOSÉ MIGUEL SAMANIEGO   
 Disputatio (2018) 8: a010  each issue in the word cloud and trace it to its sources4 to find out the contexts in which they are articulated.  
 
 
 Figure 3. Issues within AIS statements. Word cloud that depicts by size —most mentioned to least mentioned— the most pressing issues and concerns found in the sources of the statements about AIS. Issues identified are introduced in a Lippmanian Device —at the Digital Methods Initiative: tools.digitalmethods.net/beta/scrapeGoogle— along with  4  The document repository is available with the following link: https://bit.ly/2RUUAKG.  

PRACTICES OF GOVERNING AND MAKING ARTIFICIAL INTELLIGENCES  |  10   
Disputatio (2018) 8: a010  the sources. The resonance is focused on sources that correspond with sites such as blogs, news, commentary, academic accounts, press releases, forums, etc. Three plots are displayed: one with all 315 words in which the most prominent issues can be read; the other two resemble ‘zoom–ins’ of the former —180 appearances of issues in the second inset, and then only 100 in the third— to make visible a sample of less mentioned issues. Visualizations were rendered with wordclouds.com  §4. AIS layer of literatures In this section, a movement is made from statements to literatures of the AIS arena. In the case of AIS, the layer of literatures designates all the projects of standardization that have been ordered in some kind of literature, i.e., that have a documental form: publications, website entries, communications, press releases, official statements, etc. In turn, these documents represent AIS global initiatives, AI standards under development or published, AI standard series, AI guidelines, AI white papers, and so on. Thus, the layer of literatures offers an overview of AIS activities and group formations, which can be called entities as well. AIS activities and group formations that operate in several countries have been considered international (see Fig. 4.1). 
11  |  JOSÉ MIGUEL SAMANIEGO   
 Disputatio (2018) 8: a010   Figure 4.1. AIS entities map (international level). Depicts all AIS activities and group formations identified, including national strategies, global initiatives, AI standards under development or published, official statements and guidelines. Entities on the graph are color–coded and their ramifications correspond with hierarchy. The dashes–box on the left collects the initiatives according to their level of operation: international (Fig. 4.1), European (Fig. 4.2) or national (Fig 4.3). The map is crafted with Microsoft PowerPoint visualization tools.  

PRACTICES OF GOVERNING AND MAKING ARTIFICIAL INTELLIGENCES  |  12   
Disputatio (2018) 8: a010  In 2016, a report by the Committee of Legal Affairs of the European Parliament (EP) with recommendations to the Commission on Civil Law Rules on Robotics, propelled what would become a set of European initiatives (see Fig. 4.2) addressing AI matters, including standardization. The report called EU member states to “maintain control over the regulatory standards [of AI] to be set, so as not to be forced to adopt and live with standards set by others”; it called for the creation of a European Agency for AI and robotics that, among other things, would identify “standards for best practice”; and, it called the European Commission (EC) and member states “to work on the international harmonisation of technical standards, in particular together with [european Standards Developing Organizations (SDOs)] and [the International Organization for Standardization (ISO)]”5. Following the calls of the EP, three important activities and group formations regarding AIS occurred: the EC’s European Group on Ethics in Science and New Technologies (EGE) released a Statement on AI, robotics and AS on March 2018; the EU member states signed a Declaration of cooperation on AI on April 2018; and the EC issued a communication to the EP called Artificial Intelligence for Europe on April 2018.  
 Figure 4.2. AIS entities map (European level).  Individual countries have also entered the AIS arena with national strategies or initiatives of their own (see Fig. 4.3); here, relevant AIS entities from US, China, Canada, Japan, France and UK, are highlighted.  5  Available at https://bit.ly/285CBjM Last accessed on 27/01/2019. 

13  |  JOSÉ MIGUEL SAMANIEGO   
 Disputatio (2018) 8: a010   Figure 4.3. AIS entities map (national level).  As could be devised with the previous portrayal of AIS entities, the group formations dealing with this matter are headed from, mainly, North America, Europe and China. By allocating the AIS chief locations in a world map (see Fig. 5), one can see an approximation of the geographical distribution of the AIS arena. It is just an approximation in two ways. First, as warned by Venturini, search engines are not the web and the digital is not the world (2010a, p. 803); thus, other AIS activities and group formations happening in other places or times 

PRACTICES OF GOVERNING AND MAKING ARTIFICIAL INTELLIGENCES  |  14   
Disputatio (2018) 8: a010  could have been missed by the resonances, or simply not being annexed to search engines. Secondly, Figure 5 is not a map of all the places where AIS is a MoC —for example, the Institute of Electrical and Electronics Engineers’ (IEEE) Ethically Aligned Design (EAD) counts with input from countries in six continents—. Rather, Figure 5 is a map of AIS activity hubs and direction centers.  
 Figure 5. Heat map of AIS activities location. AIS initiatives and respective organizations or group formations are represented by their main geographical location. The participation of countries is color–coded by the total number of AIS initiatives/groups/standards directed from that country. In the case of global initiatives or organizations, the location is placed according to where the Chair of the project is, or to the headquarters location; with the exception of SC42 for which all members are placed on the map, because its members are the main SDOs from each participating country. Created with GIMP.  Let us turn to the most prominent issue spaces, from top–right to bottom (see Fig. 6). First, as one would expect in a layer of literatures about AIS, the top issue space is AI. Whether it be as a field or technology, AI is in the center of the AIS debate. Ethical considerations is the issue space that follows in size; the entities with stakes in the ethics of AI and/or its standardization are: IEEE entities connected to its Global Initiative, the International Communications Union’s (ITU) AI for Good, the European level initiatives, China’s AIS White Paper, US AI strategy, Canada’s AI strategy, the Japanese JSAI, the French CNIL, and UK’s CDEI. Third it comes data, with concerns that go from data governance and management to data infrastructure. Entities associated with data are the IEEE EAD and some IEEE P7000 standards, SC42 standards, the focus group ITU–T FG DPM, Acumos AI, TPC–AI, EU’s CEN/CENELEC and BDVA, CIOSC, and CDEI. Applications of AI —deployed, experimental, or envisioned— are the fourth issue space, and it is associated with SC42 standards, ITU–T 

15  |  JOSÉ MIGUEL SAMANIEGO   
 Disputatio (2018) 8: a010  Y.AI4SC, Khronos, ONNX, Acumos AI, the EP strategy, CNIL, and the Chinese initiatives —AIDP, White Paper, NAISG and AIIA—. Autonomous systems, on the other hand, is an issue for 11 IEEE related entities and for the European EGE Statement. Other prominent issue spaces include machine learning, safety, accountability, law, research, industry, security, AI ontology, economy, robotics, transparency, privacy, policy, values, leadership, interoperability, and so on and so forth. 
PRACTICES OF GOVERNING AND MAKING ARTIFICIAL INTELLIGENCES  |  16   
Disputatio (2018) 8: a010   

17  |  JOSÉ MIGUEL SAMANIEGO   
 Disputatio (2018) 8: a010   Figure 6. Issue spaces in AIS literatures. This alluvial diagram connects all initiatives/groups/standards identified to their main issues. The left side of the diagram depicts the AIS entities ordered by size, that is, by the number of AIS related issues that were identified for each. The right side of the diagram displays the issues in a similar fashion: from most mentioned to least mentioned issues. Given the difficulty to follow the diagram from left to right through the threads —because of the quantity of associations—, one way of reading the diagram is to start from the right side up, to bottom: the issues on the top are the most prominent issue spaces, and the colored threads that comprise the issue spaces indicate which entities are connected to them. To be clear, the hierarchy and size of the entities on the left do not represent the importance or size of the entity in the AIS arena; it only depicts the number of issues made explicit by the entity. Nonetheless, the quality of the hierarchy on the right side is more telling, since it is a proxy of the relevancy of an issue by its connections to several entities; in other words, the larger an issue is depicted, the larger the issue space is. The map is crafted with tools from RAWGraphs (at rawgraphs.io).  §5. AIS layers of actors and networks All entities portrayed so far have one thing in common, they act —with more or less agency within a network of AIS—. They strive to standardize this thing called artificial intelligence. In other words, we could think of the standardization process as a dynamic network comprised by actors —entities with agency on the AIS debate—. Until this point, the focused entities have been AIS group formations, series of standards, AIS initiatives, AI strategies, etc. Now, another layer to be traced is the association between entities and their own sub–networks. Thus, this section is meant to trace the entities that act upon the originally identified entities and the associations between the main actors of the AIS network. The network to be traced will be populated by whoever and whatever acts concerning AIS, regardless of entity type: organizations, technologies, humans, documents, all are considered. Nonetheless, the inclusion of humans and non–humans in the same network does not imply the impossibility to distinguish types of actors. On the other hand, since AIS related entities 

PRACTICES OF GOVERNING AND MAKING ARTIFICIAL INTELLIGENCES  |  18   
Disputatio (2018) 8: a010  are heterogeneous, then agencies will also be diverse. Action happens through association, and the ways in which entities associate with each other, matter: members of a group formation may work in new AI standards, partnerships may steer the standardization efforts of certain initiative, hierarchical relations between organizations may guide the outcomes of AI strategies, influential relations such as adherence or rivalry may shape how a standard is made, etc. Now then, for a depiction of actor/networks regarding AIS (see Fig. 7), two domains hold relevancy in representation: actors —with specified type— and their associations —with specified agency—. The disposition of actors tells a story of what is at the center and periphery of the AIS network. The nodes on the periphery form communities of their own, but are mediated into the central network by only a few actors. The threads that connect a peripheral actor to the central network are scarce, in contrast to the number of threads coming out from an actor in the central network. 
19  |  JOSÉ MIGUEL SAMANIEGO   
 Disputatio (2018) 8: a010   Figure 7. AIS actor–network (entity/link type). AIS network, comprised by 1002 actors —nodes— and 1261 associations —edges— between them. This visualization was processed with a force–vector spatialization algorithm that simulates a system of forces in which “nodes repulse each other, while edges act as springs attracting the nodes that they connect [until] reaching a balance of forces” while minimizing edge crossing; at equilibrium, the “distance between nodes become a proxy of their structural similarity” (Venturini et al. 2017). The size of the nodes corresponds roughly to their degree —number of edges attached to the node—; ‘roughly’ because they have been given minimum and maximum restraint sizes for better readability. The color code for the nodes represents the type of actor, while the color code for the edges corresponds with the type of association between two entities of the network. Node types:  Organization —including group formations, initiatives, companies, corporations, foundations, councils, committees, think tanks, NGOs, research centers, universities—;  Human;  SDO;  Standard;  Technology —including fields, systems, artifacts, infrastructures—;  Document —including resolutions, communications, official statements, national strategies—. Edge types:  Partnership —whenever they are addressed explicitly as partners—;  Membership —including directors, CEOs, editors, professors, member organizations, secretariats, committee members, Chairs, Vice–chairs—;  Hierarchy —part of, subsection of, founder of, co–founder of, authorship, parent department, parent agency, subprojects, subprograms—;  Concern —MoC, main topic, governance goals, legislation goals, standardization goals—;  Influence —liaison, recommendations, oversight, mission, reasons, 

PRACTICES OF GOVERNING AND MAKING ARTIFICIAL INTELLIGENCES  |  20   
Disputatio (2018) 8: a010  rivalry, mentions—;  Sponsor. The network was crafted with GEPHI’s ForceAtlas2, and rendered with GEPHI’s visualization tools.  The topology of the AIS network can also be read by the communities or clusters (see Fig. 8) that are formed by proximity and tightness of actors. By labeling the clusters with a code related to their central actor, the AIS network results in the following categorization —ordered from clusters with the least amount of actors to the most populated—: INCITS, Canada, France, Canada 2, IEEE, China, ITU, US, EU, IEEE 2, PAI, SC42, Khronos/ONNX, and ITU 2.  
 Figure 8. AIS actor–network (clusters). The disposition of the network is structurally identical to Fig. 7, however, nodes are differently colored in order to show clusters; colors are displayed along with their main feature in the left box. The clusters are determined by the ‘modularity measure’ algorithm in GEPHI.  Of all the actors included in the AIS network, how could we pinpoint the ones with more agency for the AIS debate and, possibly, for its outcomes or standards? In the cartographic sense, actors with the most associations will have more agential capabilities in what the 

21  |  JOSÉ MIGUEL SAMANIEGO   
 Disputatio (2018) 8: a010  specific network depicts. This is not to say that actors with fewer connections are not as large as others; it could be that an enormous organization has just a few associations in a certain network. The number of associations does not speak to the actual size of the organization, rather, it speaks to its agency in this specific network. To be connected is to be able to act, and, to be in the middle of two or more actors is to be able to mediate. If we take Figures 7 or 8 and filter out actors with relatively few associations, we obtain a network of the most prominent actors for the AIS debate (see Fig. 9). 
 Figure 9. AIS actor–network (main actors). AIS network of actors that present the most associations. This network is the result of filtering out the most marginal nodes by applying a range filter —only nodes with more than 5 edges show up—. Because the goal of this visualization is to depict the central network —main actors—, the sizes of all nodes were equated to make their labels readable. Created with GEPHI.    

PRACTICES OF GOVERNING AND MAKING ARTIFICIAL INTELLIGENCES  |  22   
Disputatio (2018) 8: a010  §6. AIS layer of cosmoses Several domains of the AIS debate could be analyzed as cosmoses. For instance, the stakes that every actor ascribes to AI, the ethics of AI, the philosophy with which the problem of AIS is approached, the envisioned futures of AI, the political positions regarding AI, and so on. These domains are explicitly or implicitly uttered by actors of the network and can shape the direction of AIS while remaining more or less stable in the literatures. In this section, two cosmoses that influence actors in their AIS quest will be tackled; namely, AI definitions and AIS logics. ‘AI definitions’ denotes the concepts of AI across AIS literatures, while ‘AIS logics’ refers to the reasons from actors to standardize AI. From the point of view of an actor, AI definitions are relevant as it is equivalent to ask ‘what is being standardized’; correspondingly, AIS logics are equivalent to the ‘why we standardize’ question. The definition of AI is not exactly the same among AIS entities; some describe AI as a field of practice, a set of applications, systems with autonomy, entities with intelligent behavior, or any combination of them (see Fig. 10). 
 Figure 10. AI definitions. It maps definitions —center column— of AI across AIS literatures —left column— and pair them with a coded type of definition —right column—. The portrayed definitions of AI are quotes from the sources, slightly modified to occupy less space and be able to represent them in a single visualization. The types of definition on the right column are (1) AI as Practice: fields, methods, theories, techniques, disciplines, weak AI, strong AI, supervised, unsupervised, etc.; (2) AI as Application: technology, ML, robotics, algorithm, program, knowledge system, capabilities, instrumentality for other fields, etc.; (3) AI as Autonomous System: independency, autonomy, decision–making, skills, self–principles, etc.; (4) AI as entity with Intelligent behavior: intelligence, passing tests, performing like humans, learning, perceiving, reasoning, being creative, planning, understanding, having goals, answering question, narrow intelligence, general intelligence, thinking, etc. Not every initiative identified in Fig. 4 is taken into account for this visualization, only those with explicit AI definitions on their literatures. Additionally, one definition is portrayed per initiative —the one stabilized in their main document—, even if within the same initiative a debate over it took or is taking place. For instance, ISO/IEC 2382 has gone through 31 revisions since 1993, but only the last definition of AI is taken into account. The map is crafted with Microsoft PowerPoint visualization tools.  

23  |  JOSÉ MIGUEL SAMANIEGO   
 Disputatio (2018) 8: a010  Paying attention to the formatting6 of the standardization issue in the literatures, a collection of AIS logics can be attained (see Fig. 11). Such logics tell stories of the reasons, motivations, interests, or convictions of different actors to strive for AIS.  
 Figure 11. AIS logics. This visualization portrays quotes of the AIS literatures that complete the statement ‘standardization is needed because…’ Quotes respect the expressions and terms used by the sources but they are re–arranged or rephrased to fit in the visualization. This map was crafted in coggle.it  6  Formatting in the sense of how issues are formatted or rendered as MoC; also known as action formats. 

PRACTICES OF GOVERNING AND MAKING ARTIFICIAL INTELLIGENCES  |  24   
Disputatio (2018) 8: a010  §7. AI standard making So, what are AI standards made of? They are socio–technical hybrids across several domains (see section ‘More than Technics’ below) made out of associations of heterogeneous entities (section ‘ANT Sensibilities’) that, in turn, address multiple artificial intelligences (section ‘AI Multiple’). Through institutionalized uniformities over space and time, AI standards attempt to govern the technics, ethics, risks, concepts and applications of AI technologies; additionally, standardization seeks to assemble AI in certain ways and brings together convening parts. In the following, some specificities and implications of the standardization of AI are presented; however, none of them touch upon the matter of standard implementation, since most of the mapped formal standards are under development, and the focus on this essay is standard making rather than implementation7.  §8. More than technics In We Have Never Been Modern (1993), Latour directs his critique towards the dichotomies of ‘modernity constitutions’, the ones that unceasingly aim to divide nature from culture. For Latour, it is enough to look at current debates such as climate change to find that such divisions do not hold up in practice; climate is a socio–technical hybrid with elements that range from the scientific to the political, the economic to the technological, the material to the discursive, and so on. These ‘objects–discourse–nature–society’ (Latour 1993, p. 144) abound in the contemporary world, and it should be clear by now that the standardization of AI is, also, a hybrid debate. Years ago, throughout my engineering studies, the question of standardization was for me a synonym of making technics possible: I thought of standards solely through their dimensions of technical conventions, specifications, requirements, efficiency, industrial safety or design. To say that standardization spills over other domains of collective life —in their awakening, uptake and implications— would have been certainly surprising for the undergraduate me. Nonetheless, that is hardly a surprise for anyone; sure, technoscientific artifacts are political (Winner 1986). But as the idiom goes, the devil is in the detail; then, still I wonder in which specific ways is AI standardization (AIS) political? Following, some ways in which AIS is more than a technical matter are presented, grounded in what was traced in the atlas. First and foremost, there is an explicit scattering of the matters of standardization that surround AIS. Historically (see Fig. 1.1–1.4), AIS is casted as a matter of research, application, ontology, adoption of AI technologies, ethics, risk and safety, benchmarks, and others. In other words, there has been an expansion of the domains that AIS is expected to cover, starting with ‘research’ and then intensifying in other areas over the years. As one can see,  7  The matter of implementation is one of the main processes of standardization; it is an “active, time —and resource— intensive process” because it requires integration of different levels, auxiliary systems and local flexibility for the standard’s success (Timmermans and Epstein 2010, p. 81). Following further the line of inquiry of this work would require to pay attention to the implementation and outcomes of AI standards. 
25  |  JOSÉ MIGUEL SAMANIEGO   
 Disputatio (2018) 8: a010  these matters of standardization are not solely technical; rather they cover a wide array of governing modes for AI fields with a focus on (see Fig. 2): AI applications; politics; technical reasons that demand standardization; economic benefits; solving challenges presented by AI technologies and their implications; matters of ethics, policy and law; and the study, advancement or research of AI. To give an example of the issues (see Fig. 3) that these categories elicit, a few concerns of the AIS network are mentioned, such as: the future of work, accountability, data ecosystems, best practices, AI capabilities, public trust, strategy, autonomous weapon systems, etc. The immediate political question to AI standardization and the transformation of its related issues, is: who gets to participate? The organizations with the most influence in the AIS debate are mainly headed from North America, Europe and China (see Fig. 5). It is not that the Global South does not participate at all, but clearly the standardization activity hubs are located in the Global North (plus China) and thus occupy a privileged position for AIS; this raises the complementary political question: who is left out of the AIS debate or who chooses not to partake on it? The specific reasons, motivations, interests, convictions or ideologies that drive the urge to standardize AI can be distilled with the following logics (see Fig. 11): AI development, technological progress, safety and accountability, interoperability and reliability, ethical governance, security and privacy, leadership, instrumental applications, innovation and market competition, open formats and AI environments. From here, one could think of two categories that we will label progressivism —development, progress, interoperability, reliability, security, leadership, applications, innovation, markets, formats and environments— and accountability —safety, accountability, ethics, security and privacy—. Of course, these two characterization labels do not necessarily exclude each other; often, the same group formation wields logics from both. Whenever there are several logics of governance and action, the debate is political insofar other alternatives may be available: progressivism is just one way to shape, and to be shaped by, AI; and accountability matters can be rendered otherwise, depending on who you ask. In Science and Technology Studies (STS) literature, ‘timescapes’ address a hegemonic technoscientific future–oriented culture: sometimes it shows as a ‘progressivist’ imperative to advance, or as future–oriented practices that depend on expectations, or as a focus that is driven by technoscientific innovation on a permanent state of awaiting for imminent breakthroughs (Puig de la Bellacasa 2015, pp. 693–694). Certainly, the logic of progressivism is dominant in the AIS debate where standards come to be seen as means to reach an elusive future of artificial general intelligence (AGI) —e.g., OpenAI—, AI progress —e.g., US NAIRD—, 5G networks —e.g., ITU—, or AI leadership —e.g., AI for Europe—. The intricacy of AIS logics lies in the variety of its manifestations, a sort of spectrum that goes from hard progress to mild progress. For instance, a milder progressivism can be seen in sentences like: “teams working on developing AGI systems should be prepared to put significantly more effort into AI safety research as capabilities grow” (IEEE Global Initiative 2017, p. 77); this type of statement still implies technoscientific innovation and futurity, but, through a slower 
PRACTICES OF GOVERNING AND MAKING ARTIFICIAL INTELLIGENCES  |  26   
Disputatio (2018) 8: a010  pace that attends to risks. And, at that point8, many of the AIS initiatives turn to a logic of accountability. Being attentive to situatedness and partial perspectives (Haraway 1988), accountability of our world making practices turns out to be contextual and permanently at stake (Kenney 2015); hence, as with the spectrum and alternatives of progressivism, accountability is political since it puts forward certain ethics, ideologies, frames or philosophies over others9. Lastly, there are three other not so obvious ways in which the AIS debate is a socio–technical hybrid with political ramifications. First, in what we termed China, US, EU, SC42 and Canada clusters (see Fig. 8), one can see States10 conferring AI standards a kind of “soft regulation”; in turn, non–governmental organizations, like ISO, produce international standards that will be “adopted as national standards” (Timmermans and Epstein 2010, pp. 76–77; p. 83) by member National SDOs. Second, AIS can be seen as part of the infrastructural work to enable AI. Standards eventually become “part of the taken–for–granted technical and moral infrastructure of modern life” (Timmermans and Epstein 2010, p. 71). Infrastructures have long been observed to be anything but neutral; in their context, standardization is a “political process: the preferences, values, and practices of one group are adopted and supported above others” (Stola and Bowker 2017, p. 545). Infrastructural thinking also hints to the ways in which AI facts and artifacts come into being: standards and their local implementation/interpretation are part of an apparatus (Barad 1998) that, along with other agents, enable a specific type of AI artifact or concept. Third, and connected to the previous ramification, AIS aims not only to govern the object, but also to create it in certain ways. This is better understood by ontological politics, a term coined by Annemarie Mol, of which she says: “reality does not precede the mundane practices in which we interact with it, but is rather shaped within these practices. So, the term politics works to underline this active mode, this process of shaping, and the fact that its character is both open and contested” (Mol 1999, pp. 74–75). No doubt standardization is an active mode for shaping the future of AI, and considering the many alternatives and initiatives of AIS, it certainly is contested as well.     8  When the debate shifts to safety, privacy, transparency, responsibility, etc. 9  For example, in an IEEE report about the ‘request for feedback’ of the EAD, commenters make remarks on the assumptions, philosophies, ethics and frameworks of the previous EAD version. They call attention to the inclusion of other fields and non–western principles and philosophies. One instance of the report asserts that a Buddhist doctrine would strive for a commitment to an open source AI, since “non–attachment means minimizing profit, pride, or prestige to only those levels necessary to sustain one’s activities and improvements towards an ideal, ethical, goal” (Jordan 2017). One can imagine how underlying ‘principles’ may have world making impacts, since, commercial AI is definitely not the same as open source AI. 10  Through National SDOs responding to Councils, Federal Agencies, or Parliaments, depending on the country. 
27  |  JOSÉ MIGUEL SAMANIEGO   
 Disputatio (2018) 8: a010  §9. ANT sensibilities The ‘social’ is understood in actor–network theory (ANT) as the associations or relations among heterogeneous elements that can be assembled anew under a given state of affairs (Latour 2005, p. 5). Or, as John Law puts it, ANT is “radically relational” and a “tool kit for talking about heterogeneous relationality” to map how objects or actors are “shaped in its relations” (Law 2017, p. 41). The themes running in ANT descriptions are heterogeneity and relationality, and the themes running in its methods are attention to materiality and empiricism; or, in a single sentence: the world is “materially heterogeneous” (Law and Singleton 2015, p. 4). We have seen already some logics behind AIS; but, how are AI standards created? ANT sensibilities can shed light on this topic. Figures 4.1 to 4.3 indicate the relationality within the actor–network that give way to AIS; that is, the associations, in this case, group formations, that strive to assemble AI anew. Group formations connected to IEEE, ISO/IEC, and ITU, among others, are characterized by their transnational activities and international–standard–uptake expectations. On the other hand, group formations like the European AI Alliance and related processes such as the craft and promotion of AI for Europe, hint to a relationality at the European level. Furthermore, standardization initiatives that work at the national level of certain countries are connected to their respective States with national strategies. Although many of the mentioned organizations pre–exist the current state of affairs, AIS activities come from group formations within or outwards such organizations; those institutional associations are specific to the AIS task. AI standardization follows a dynamism of abundant creations of committees, subcommittees, initiatives, programs, subprograms, bi–national groups, SDOs gatherings, national strategies, industry alliances, research groups, partnerships, open publications or debates, companies and enterprises, etc. Sure, ISO and IEC existed before AIS, but the complexity and size of its subcommittee for AI issues (SC42) is outstanding; with high stakes and many stakeholders, AIS takes a lot of work.  AIS group formations are heterogeneous in organizational form and content: diverse group formations with diverse standards. Some formations can be pushed strongly and quickly established, but still their goals, scope or methods take time and work to be specified. Take the case of the European initiatives: the EP commands group formations for AIS, but then standardization has to be figured out; at first, it is not clear which directives of the EU will change accordingly, or which technical committee will tackle the challenge, or how. This points to one thing: AI standardization is not only about governing and assembling a technology, it is also about assembling the groups, conventions and their audience. Conventions need convening parts, and those parts have to be brought together. The actors of the AIS debate and their agencies are heterogeneous. We have identified entities (see Fig. 7) that shape AIS, namely, organizations, humans, technologies and documents. These actors are associated through partnerships, memberships, hierarchical relations, concerns, influences, and sponsorships. The most influential actors of the AIS network (see Fig. 9) are the ones related to: IEEE, ITU, and ISO/IEC initiatives and standards; to the technologies and fields of standardization, such as AI, autonomous systems (AS), 
PRACTICES OF GOVERNING AND MAKING ARTIFICIAL INTELLIGENCES  |  28   
Disputatio (2018) 8: a010  robotics, machine learning (ML), data, vision processing; to the AI industry and standards, like PAI, Khronos, ONNX, Open AI and Acumos AI; and to the Chinese, European, US, Canadian, UK, and French strategies and standards. In the following example regarding SC42, consider how heterogeneous —in type, location, agency, size, etc.— these entities and associations are: SC42’s parent organizations are IEC and ISO, and their infrastructure partly enables it; European Standard Organizations such as CEN and CENELEC “have strong ties”11 with ISO, IEC and SC42; SC42’s chairperson Wael William Diab has been/is senior technical director at Broadcom and Huawei; the last plenary meeting of SC42 was held in Beijing, as for the Chinese government it is important to have a leading role in global AIS; the secretariat that oversees SC42 is the US ANSI; SC42 matters of standardization range from big data reference architecture to AI terminology; stakeholders of all kinds comment on SC42: from the media to the academia, and I comment on it too with maps and descriptions, not as mere intermediary, but transforming the message with my own set of assumptions; many national SDOs are members of SC42 and give input to the creation of its standards; and so on and so forth. In one long sentence, we go from infrastructure to ‘European harmonized standardization’, to humans with associations to the stakeholder industry, to meetings with high stake politics, to hierarchical relations, to hosting organizations, to standards in the making, to technoscience, to situated representations of SC42, to participating countries. Heterogeneous relationality happens also through issue spaces. Issues that are shared among entities of the AIS arena include (see Fig. 6): AI, ethical considerations, data, applications, AS, ML, safety, etc. Entities deliberate on issues that they have in common with other entities, and thus, in these spaces, issues may be assembled anew. However, because not all the entities in the AIS debate share all the issues identically, one could expect the concerns —and arguably, the outcome standards— to be different —and sometimes overlapping— across the network.  §10. AI multiple As stated, AIS assembles convening parts; attempts to govern AI; and supports the creation of AI technologies. Hence, in the question of standardization are implied other questions such as why we standardize, how we standardize, and what we standardize. The ‘what’ question has two qualities to it: on the one hand, one needs to know ‘what’ is being standardized to govern it consequently —an example would be terminology and concept standards such as ISO/IEC AWI 22989—; on the other hand, one standardizes with a ‘what’ in mind to make that future happen —for instance, in State endeavors like the Chinese White Paper that aligns to the national strategy—. There are also standards that tackle more explicitly both qualities of ‘what’, descriptive and imperative; for instance, the IEEE P7007 Ontological standard for ethically driven robotics and automation systems. With this in mind, the simple question of what is being standardized in AIS becomes complex. Simply put, what is being standardized is AI; but the problem is that the AIS debate is heterogeneous in its distinct group formations  11  In CEN/CENELEC webpage, available at https://bit.ly/2C7Q02C. Last accessed on 27/01/2019. 
29  |  JOSÉ MIGUEL SAMANIEGO   
 Disputatio (2018) 8: a010  and locations, goals, methods, content of standards, actors, agencies, issues and MoC. Then, new questions arise about the specific artificial intelligences that will be done, or that are being enacted, through these webs of relations. To think about this variety of artificial intelligences, the term ontological multiplicity is convenient. It designates the enactment or performance of an object that is different itself in different webs of relations. Reality “is manipulated by means of various tools in the course of a diversity of practices […] But as a part of such different activities, the object in question varies from one stage to the next […] Instead of attributes or aspects, they are different versions of the object, versions that the tools help to enact” (Mol 1999, p. 77). John Law and Vicky Singleton (2015) capture these ideas by stating that realities ‘are done in practices’: they are ‘made to be’ something. In the face of Western common sense, ontological multiplicity of things like foot and mouth disease might sound counterintuitive (Law and Singleton 2015, p. 8), since they are things often deemed with inherent and essential properties or pertaining to the domain of ‘nature’. Nonetheless, the mentioned authors make strong cases for the multiple ontologies of such things; in turn, these multiplicities can coexist side by side and be related to one another. Now, it is perhaps easier to wrap our mind around the ontological multiplicity of AI because, also in ‘Western common sense’, “the point of technology and politics has always assumed that reality can be otherwise, or changed” (Mol 1999, p. 75). Thus, it is not as provocative to say that AI is multiple than to say that atherosclerosis is (Mol 2002). However, pointing out the multiplicity of AI has returns in the AIS debate that may deserve more attention: first, in the process of standardization not only the ontology of AI is at stake but other related entities may not leave unchanged in their respective enactments; second, standardization processes do not always take AI multiplicity into consideration; third, reflecting on the practices of enacting one AI over another, matters. In what follows, these three points are expanded upon. First, AI and AIS issues are multiple, they are done in practices. When we think of two stages for the performativity of AI, say, one that aims to a Digital Single Market and a Data Economy12, and one that aims to standardize AI as an open source framework for applications13; we certainly are hinting to different enactments of AI. It is not the same AI that is lying in a European Parliament member’s folder over their desk, as the one that is being sold through a digital store by Acumos AI. One of them is made to be a solution for Europe’s competitiveness in the AI landscape aligned to European values; the other one is made to be a commodified service for facial recognition or image classifiers. To be sure, I haven’t been in the actual sites of AI practice mentioned in this work, or physically near to the humans and non–humans that enact artificial intelligences; therefore, it would escape the scope of this essay to pinpoint specific ontologies of AI or to state the nuances of the practices of so many actors. Nevertheless, some dimensions that would collaborate in the enactment of artificial intelligences have been described already: matters of standardization, issues and MoC, group  12  Reference to the AIS European initiatives. 13  Reference to the Acumos AI initiative from the Linux foundation. 
PRACTICES OF GOVERNING AND MAKING ARTIFICIAL INTELLIGENCES  |  30   
Disputatio (2018) 8: a010  formations, location of AIS activities, actors, AI definitions and AIS stakes. Then, at least it is safe to assume that in the midst of such heterogeneous relationality, more than one AI is being enacted. In the previous two sections, most of those dimensions have already been touched upon, so let me tackle the one not yet mentioned: AI definitions (see Fig. 10). Literatures that correspond with AIS initiatives define AI in heterogeneous, yet not necessarily incommensurable, ways: AI is a field of practice, an application, a system with autonomy, an entity with intelligent behavior, or a combination of them. Here, AI is being practiced differently in a conceptual level, what may lead to different practices in an instrumental level. These are not just different attributes of AI, rather they signal to multiple facts and artifacts; for example: machine learning, intelligent machine, autonomous software, passer of the Turing Test, knowledge based system, diagnose system, research discipline, industry solution, adaptive machine that models the world, human–like artifact, weapon, capability, commodity, weak/strong AI, etc. All of them are AI. Some AIS related issues designate objects that, in their webs of relations, may enact certain realities through the associations described. In this sense, it is not only expected that standardization will multiply realities for AI —and hinder others—, but maybe also for some of its related issues. Mol has pointed out to how in the clinical framing of a disease, other categories besides the disease —for example gender— are performed in ways that enact divisions (Mol 1999); and this is precisely the complexity of ontological multiplicity. Similarly, in our case we have seen categories that are themselves practiced in one way or another in connection to AIS. For instance, ethical considerations, innovation, security; what are those? Nobody would say that the ethics of one group formation is exactly the same as another; or that every actor practices innovation likewise; or that security signifies the same thing in every country of the AIS debate. One thing is for sure; some more, some less, but no one is leaving the controversial arena of AIS completely untouched: whether you are a scientist in a standardization committee, an AI industry stakeholder, a doctrine of transparency for algorithms, a data set for ML training, a person measured with facial recognition and credit score systems, an international standardization organization, a discourse —whether fearful or hopeful— of human–level AI. Secondly, AI multiplicity matters for standardization: the evident argument is that standardizing different things requires different standards. Of course, the ontology of the thing to be standardized is not the sole dimension that determines the final standard; each standard has its own history and motivations, and formal standards may tackle different aspects such as design, terminology, performance or procedures (Timmermans and Epstein 2010). However, one cannot deny the importance of the question ‘what is being standardized’ in the standardization process. Take the case of IEEE P7000 Model Process for Addressing Ethical Concerns During System Design. In a meeting minute regarding the creation of the standard (P7000 Working Group 2018), concerns are voiced about the standardization workflow and its tension with the ontologies described in the IEEE glossary (see Fig. 10 for the definition of AI according to the glossary). Members of the group address the inconsistencies between the working group concepts, the ones from other subgroups of the IEEE standard 
31  |  JOSÉ MIGUEL SAMANIEGO   
 Disputatio (2018) 8: a010  series, and the ones in the glossary; others encourage to stick to the glossary for all the research and cited references that went into it; others claim that it would be “inappropriate to rewrite the dictionary based on P7000 view”; others call for consistency of ontologies across the P7000 series; others request the creation of an ontology and terminology subgroup, given that the working group could not move “forward if [they] do not know what [they] are talking about”; others ask to avoid taking the glossary as starting point; finally, the minute ends with a “Motion to create an Ontology Subgroup” (P7000 Working Group 2018). This is but one example among the many group formations of AIS; however, it hopefully serves as an illustration of the importance of AI multiplicity in the standardization process. To the third point of multiplicity in the AIS debate: reflecting on the practices of enacting one AI over another. Standards are one type of relational web that —will— contribute to how AI is practiced; that is, standards stage the object in certain ways. So, the significance of studying standards is not only self–referential —the study of standards—, but it also helps in understanding the practices that make up the AI multiple. At the current state of affairs of AIS, the main endeavor is towards standard creation —and partly implementation—; one could say that AIS is right now the attempt to enact AI according to national strategies, corporate logics, professional associations and stakeholders concerns, cosmoses, etc. To designate these dynamics, the concept of ontological politics (Mol 1999) was already cited. Consider the stakes for AIS ontological politics: if stakeholders portrayed in this work are right in how important AI will be in the next years for security, markets, applications in the health care industry or in transportation, research, benefits or detriments to humanity, etc.; then, the becomings14 of AI are certainly political and high stake. Allow me to give an example of ontological politics in the AIS debate. The Statement on AI, Robotics and AS by the European Group on Ethics in Science and New Technologies (EGE), addresses growingly autonomous technologies; that is, “ever smarter systems […] that exhibit high degrees of what is often referred to as ‘autonomy’” (2018, p. 7). Regarding these technologies, the EGE critiques the use of the word ‘autonomous’ in AI discussions. For the EGE, the term should refer to the capacity of humans to “legislate for themselves, to formulate, think and choose norms, rules and laws for themselves to follow” (2018, p. 9). Since autonomy is deeply intertwined with the rights of people to set their own goals —and to human dignity—, they argue that, in the ethical sense, autonomy can only be attributed to humans. Furthermore, the group states that human dignity and agency “entail the features of self–awareness, self–consciousness and self–authorship” (2018, p. 9). This ethical ontology of autonomy and self–authorship clashes with the ontology practiced by cyberneticians15, who  14  I am borrowing Donna Haraway’s term becoming. She deploys it in a different context. Namely, for the act of becoming with companion species through material–semiotic encounters; which means that inter–species come together to perform, to become with each other through entanglements that are brought about by histories, cultures, technologies, organisms, relationships, etc. (Haraway 2008). However, because ‘companion species’ does not refer solely to living organisms, it makes it tempting to use the term in the present context. 15  Even though the EGE does not mention cybernetics in their Statement, the critiqued ontology of autonomy definitely presents parallels with the cybernetic tradition. For an analysis on the discourses 
PRACTICES OF GOVERNING AND MAKING ARTIFICIAL INTELLIGENCES  |  32   
Disputatio (2018) 8: a010  understand autonomy and self–organization —autopoiesis— as features of cybernetic systems in general, whether they are machines, living organisms/humans, organizations, or a combination of them. The EGE replies: it is “somewhat of a misnomer to apply the term ‘autonomy’ to mere artefacts, albeit very advanced complex adaptive or even ‘intelligent’ systems” (2018, p. 9). For them, an ethical turnaround of the ontology of ‘autonomy’ means that the ways we control machines should be separated and different to the management of humans and human data; the logical connection they make comes from the notion that humans possess rights that are not interchangeable with machines: “‘autonomous’ management of human beings would be unethical” (EGE Statement 2018, p. 10). This final remark seems to oppose the notions of cybernetic governance, in which, autonomy “depends on the balancing of two feedback flows of data: one from the system’s performance [including humans in the loop] in the present and in its environment; the other from the system’s past, in the form of symbols recalled from its memory” (Deutsch 1963). For the EGE, even if it were technically conceivable, this kind of cybernetic governance is not compatible with “European core values” (EGE Statement 2018). The political move of the EGE is not necessarily to deny the ontology of AI as autonomous system; rather, they present a different possibility for the ontology of autonomy based on ethical grounds, and that is explicit ontological politics. After presenting three ways in which the multiplicity of AI matters for the AIS debate, one final clarification is needed about ontological multiplicity. One cannot just dream a new reality —say, a new AI— because “realities are practiced into being in heterogeneous networks of relations which takes a lot of effort, many resources, and a great deal of hard work” (Law and Singleton 2015, p. 11); that implies that “the world isn’t a different place every morning” (Law 2017, p. 42). Figure 10 might give the impression that it is enough to enunciate a definition of AI to bring it into the world, to make it real. That is not the case; at least, it is not the whole story. Of course, immutable mobiles (Latour 1986) like the documents that utter those definitions, influence, and will continue to do so, the making of facts and artifacts. But surely, this is just one part of the huge network of actors and agencies that goes into enacting ontologies of artificial intelligences. That being said, even if we would only pay attention to the documents that state formal definitions; even there, we would find that each one of those documents carry behind ‘hard work’ processes, and that they are not a thing out of nowhere. Documents taken into consideration in this essay come about from vast group formations —e.g., EAD—, deliberations —e.g., AI for Europe—, international collaboration —e.g., ISO/IEC standard series—, and large national endeavors —e.g., China’s AI White Paper—. Therefore, the AI definitions of Figure 10 are not just ‘dreamed realities’. Besides the creation process of AIS documents, their outcomes matter as well; in classic ANT, great importance is placed “on documents and their materiality in the accumulation of work and the enrollment of allies” (Shankar et al. 2017, p. 66). Thus, equally significant is the becoming of a document in the AIS debate, and its agency for the governance and ontologies of AI —that is the reason why AIS  and practices of cybernetics, and particularly on the cybernetic ontology of autonomy, see Hayles (1999). 
33  |  JOSÉ MIGUEL SAMANIEGO   
 Disputatio (2018) 8: a010  documents were included in figures 7, 8 and 9, as actors—. The following quote captures the role of AIS documents, AI concepts and AI definitions, in the enactment of the AI multiple: Typically […] we draw a line between reality on the one hand, and what we know about it on the other. To put it in the jargon of the philosophers, we draw a division between ontology (what there is) on the one hand, and epistemology (what we know about reality) on the other. But this is precisely what actor network theory does not do, because in ANT realities are done along with representations (Law and Singleton 2015, p. 9).  §11. Conclusions We have said that AI standards are socio–technical hybrids across several domains, made out of associations of heterogeneous entities, which address multiple artificial intelligences. AI standardization (AIS), through institutionalized uniformities, simultaneously brings together convening parts and their audiences, intends to assemble AI technologies in certain ways, and attempts to govern AI fields. AI standards bear high stakes because they are more than just technical matters: they are political, they take a lot of socio–technical and infrastructural work, and they contribute to how AI is practiced. As Timmermans and Epstein assert: contemporary “life increasingly depends on the creation, institutionalization, use, and dissemination of diverse kinds of standards” (2010, p. 70). The significance of studying AI standards is not only self–referential, but also inquires the practices that make up artificial intelligences and its related issues; and in those practices, humans and non–humans are interwoven. Some implications of the socio–technical hybrid and the political ramifications of AIS, are: the notorious expansion of the matters of standardization related to AI in the last years; the dominance over the AIS debate by North America, Europe and China; the pervasion of progressivist and accountability AIS logics, which in themselves, are contingent and contested; the use of AI standards as soft regulation devices; the non–neutral infrastructural work that AIS entails; the governing and imperative dimensions of AIS. The creation of AI standards was traced to numerous associations of humans and non–humans, that is, group formations that strive to assemble AI anew; AIS group formations are heterogeneous in form and content. Moreover, the actors that compose those groups are heterogeneous as well: organizations, humans, technologies, documents; and the associations between them, and the agencies over one another, are done mainly through partnerships, memberships, hierarchical relations and influence. Heterogeneous relationality occurs also in issue spaces; one could expect matters of concern to be different —and sometimes overlapping— among entities, affecting in this way the matters of standardization across the network. In these webs of relations, AI and related issues are ontologically multiple; they are done in practices. The dimensions that collaborate to the enactment of multiple artificial intelligences —whether they be fact or artifact— are, for instance, matters of standardization, MoC, heterogeneity of group formations, location of AIS activities, actors’ associations, AI definitions and AIS stakes. Thus, heterogeneous relationality within AIS multiplies the 
PRACTICES OF GOVERNING AND MAKING ARTIFICIAL INTELLIGENCES  |  34   
Disputatio (2018) 8: a010  realities of AI and its related issues. AI multiplicity matters for standardization since one cannot underestimate the question of ‘what is being standardized’ in the process. Ontological politics designates the open and contested active mode of shaping reality through practices (Mol 1999). To bring the last argument across, let me quote two other authors that, in a way, share these ideas of politics. First, we have Bruno Latour with what he terms political epistemology. It comes from the “intuition that associations are not enough, that they should also be composed in order to design one common world”; here, the political and scientific tasks are connected to having “new entities detected, welcomed, and given shelter” (2005, p. 259). After mapping and deploying the associations that make up a certain state of affairs, Latour proposes an exploration for re–assembling those associations “in a satisfactory form”; and with this purpose, the “distinctive touch” of ANT is to “highlight the stabilizing mechanisms so that the premature transformation of matters of concern into matters of fact is counteracted” (2005, p. 261). In other words, Latour’s proposal is to slow down the stabilization of facts by taking seriously relationality and multiplicity, and to include as many entities as possible in our politics. ‘Slowing down’ brings us to another proposal of politics; called cosmopolitics and made by Isabelle Stengers, the proposal is summarized in the following quote:  [S]low down the construction of this common world, to create a space for hesitation regarding what it means to say ‘good’. When it is a matter of the world, of the issues, threats and problems whose repercussions appear to be global, it is ‘our’ knowledge, the facts produced by ‘our’ technical equipment but also the judgments associated with ‘our’ practices that are primarily in charge. [The cosmopolitical proposal] is ‘idiotic’16 in so far as it is intended for those who think in this climate of emergency, without denying it in any way but nonetheless murmuring that there is perhaps something more important. The cosmos [...] refers to the unknown constituted by these multiple, divergent worlds and to the articulations they could eventually be capable. This is opposed to the temptation of a peace intended to be final, ecumenical” (2005, p. 995). Commenting on Mol, Latour and Stengers, authors Law and Singleton state that those political proposals are a “shared agenda that struggles with the message that after the heterogeneity and the relationality […] (1) there is no such thing as a single reality; (2) that realities cannot simply be dreamed up but take work and care […]; and (3) that realities and politics or normativities are all wrapped up together” (2015, p. 16). To put it differently, they are proposals of explicitness of multiplicity, practices, and politics in technoscientific debate. The awareness of these dimensions should bring about objects —facts and artifacts— more attuned to the specific settings of their implementation. Standard makers, technologists, and other stakeholders could be more accountable of the artificial intelligences they bring into the world, their own practices and their politics. Perhaps Martha Kenney’s (2015) ‘empirical and  16  Stengers employs the figure of ‘the idiot’ advanced by Deleuze: one who “slows the others down, who resists the consensual way in which the situation is presented and in which emergencies mobilize thought or action” (Stengers 2005, p. 994). 
35  |  JOSÉ MIGUEL SAMANIEGO   
 Disputatio (2018) 8: a010  political accountability’ could advance an understanding of accountability as the one just hinted at. ‘Empirical and political accountability’ highlights the practices of researchers and technologists and offers a practical way of accounting for our world making practices. This accountability ethos of feminist theory within STS, prompts to be reflexive about our own technoscientific practice and to consider revisiting the objects we generate. Standard makers, technologists, and other stakeholders, should be accountable for the artificial intelligences they bring into the world by “tracking, highlighting and narrating” their materialized heterogeneous relationalities in order to avoid the “illusion of selfevidence” (Kenney 2015, p. 767). Thus, this type of accountability stands as a recommendation for participative storytelling. In practical terms, this would mean to open up the process of standardization with storytelling for scrutiny: “a kind of accounting where ‘show your work!’ acts as a reminder to resist naturalization” (2015, p. 767). Perhaps in the AIS debate, IEEE’s EAD and the P7000 standard series get closer to this ethos of accountability; two versions of the former have been published —and a third one is on the way— after revisiting it and making it public for discussion and feedback, and as for the latter, the meeting minutes and drafts of the process —with insecurities and all— are made available for the 11 standards being developed. Through standardization, actors expect many things: AI development, technological progress, safety and accountability, interoperability and reliability, ethical governance, security and privacy, leadership, instrumental applications, innovation and market competition, open formats and AI environments. These expectations are said to tackle risks, inequalities, economic growth, and untimely, to entail benefits for humanity. Perhaps, if we would be dealing with a single AI, the path for attaining these goals would be clearer. However, we should not assume a single ontology, neither for AI nor for its related issues; rather, attention is needed to how artificial intelligences and other issues are practiced into reality. With the insights of ontological politics, political epistemology, and cosmopolitics, realities seem to be ‘softened’ and, therefore, it is possible to “imagine better alternative realities” (Law 2017, p. 43). In this sense, a preferable standardization process would be one that: inquires the ontologies of AI and related issues and recommends the ‘better’ alternatives; one that slows down the stabilization of concerns into facts —that is, into artificial intelligences— by including as much entities as possible; one that creates the spaces to debate ‘what it means to say good’, or in this case, what it means to prophesy ‘benefits for humanity’; and, one that reflects on itself as a ‘non–innocent’ (Kenney 2015) practice that actually takes part in the materialization of some artificial intelligences over others. Assuming the AI multiple would open up new possibilities, because the starting point of the discussion would change from ‘just having different perspectives’ on AI —which assumes there is one correct answer to the matter of standardization— to an awareness of standardization as a world making practice. This mode of attention forces us to focus on materiality and relationality to be explicitly consequent to AI related issues; instead of, for instance, just telling hyped narratives of progress. Of what possibilities are we talking about? The example of ontological politics in the EGE statement (see previous section) is one instance of exploration of ontological possibilities for autonomy, which is a big issue in the 
PRACTICES OF GOVERNING AND MAKING ARTIFICIAL INTELLIGENCES  |  36   
Disputatio (2018) 8: a010  AIS debate. As an extension of that example, consider Tal Zarsky’s The Trouble with Algorithmic Decisions, where the author maps the most salient concerns regarding automated decision–making; namely, the tensions between efficiency and fairness (Zarsky 2016). According to the author, what exacerbates this tension is the automated and opaque attributes of automated decision–making. He proceeds to point out available solutions to the tension, that range from “abandoning algorithmic decision making entirely, applying forms of transparency, [to conducting] audit trails to the algorithmic process” (2016, p. 130). Connecting both instances —the proposal of the EGE statement and the mentioned tension between efficiency and fairness—, one can see how the political proposals by Mol, Latour and Stengers, could help decide over these matters. On the one hand, there is a diversity of options pointed out by Zarsky, and where there are options there are politics and possibilities, both for public policy as for standardization. Yet, mapping the diversity of options could be complemented; a step further is to call into debate the very practices and ontologies that make up ‘efficiency’, ‘fairness’ and ‘algorithmic decision making’. In other words, in the debate of AIS, ‘AI’, ‘efficiency’ and ‘fairness’ are up for grabs, and the better way of standardizing these objects would be to consider their situatedness and multiplicity; to keep them as matters of concern for as long as it takes; to revisit them numerous times and in multiple sites. To end, I want to bring attention to the tittle of Timmermans and Epstein’s (2010) article: “A world of Standards but not a Standard World”. This carefully crafted sentence somewhat encloses this discussion: on the one hand, standards are becoming ubiquitous in AI fields; on the other hand, the heterogeneousness that these standards attempt to govern cannot be standardized fully. So, what do we make of the hard work that all the actors are putting into it? Well, standardization could perhaps be better thought as a world making practice; but just as one column of that multiple edifice we call ‘AI’. Thinking of AIS as an AI making practice, as an ethico–political process, and also as the active mode of shaping its related issues, may awake in us stronger concerns of ontological responsibility and accountability: what do we want to standardize? Even better, which ontology is worth standardizing?  References Barad, Karen (1998). “Agential Realism. Feminist Interventions in Understanding Scientific Practices”. In: The Science Studies Reader, edited by Mario Biagioli. New York: Routledge, pp. 1–11. Barbrook, Richard (2007). “New York Prophecies. The Imaginary Future of Artificial Intelligence”. Science as Culture 16, no. 2: pp. 151–167. doi: 10.1080/09505430701369027 Brown, Nik (2003). “Hope against Hype. Accountability in Biopasts, Presents and Futures”. Science Studies 16, no. 2: pp. 3–21. Couldry, Nick (2017). “The Myth of Big Data”. In: The Datafied Society. Studying Culture through Data, edited by Mirko Tobias Schäfer and Karin van Es. Amsterdam: Amsterdam University Press, pp. 235–239. doi: 10.1515/9789048531011-019 Deutsch, Karl (1963). The nerves of government. Models of political communication and control. New York: The Free Press. EGE Statement (2018). “Statement on Artificial Intelligence, Robotics and ‘Autonomous’ Systems”. Edited by Publications Office of the European Union. European Group on Ethics in Science and New Technologies. 
37  |  JOSÉ MIGUEL SAMANIEGO   
 Disputatio (2018) 8: a010  Luxembourg. Haraway, Donna (1988). “Situated Knowledges. The Science Question in Feminism and the Privilege of Partial Perspectives”. Feminist Studies 14, no. 3: pp. 575–599. doi: 10.2307/3178066 Haraway, Donna (2008). “When Species Meet. Introduction”. In: When Species Meet, edited by Donna Haraway. Minneapolis: University of Minnesota Press, pp. 1–42. Hayles, Katherine (1999). How We Became Posthuman. Virtual Bodies in Cybernetics, Literature, and Informatics. Chicago: The University of Chicago Press. doi: 10.7208/chicago/9780226321394.001.0001 IEEE Global Initiative (2017). “Ethically Aligned Design. A Vision for Prioritizing Human Well–being with Autonomous and Intelligent Systems”. 2nd ed. Available at https://ethicsinaction.ieee.org/. Last Accessed on 27/01/2018. Jordan, Sara (2017). “Becoming a Leader in Global Ethics. Creating a Collaborative, Inclusive Path for Establishing Ethical Principles for Artificial Intelligence and Autonomous Systems”. Edited by The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. Available at https://standards.ieee.org/industry-connections/ec/autonomous-systems.html. Last Accessed on 27/01/2018. Kenney, Martha (2015). “Counting, accounting, and accountability. Helen Verran's relational empiricism”. Social Studies of Science 45, no. 5: pp. 749–771. doi: 10.1177/0306312715607413 Latour, Bruno (1986). “Visualisation and Cognition. Drawing Things Together”. In: Knowledge and Society. Studies in the Sociology of Culture Past and Present, edited by Henrika Kuklick. Greenwich: Jai Press, pp. 1–40. Latour, Bruno (1993). We Have Never Been Modern. Cambridge–Massachusetts: Harvard University Press. Latour, Bruno (2004). “Why Has Critique Run out of Steam? From Matters of Fact to Matters of Concern”. Critical Inquiry 30, no. 2: pp. 225–248. doi: 10.1086/421123 Latour, Bruno (2005). Reassembling the Social. An Introduction to Actor–Network–Theory. New York: Oxford University Press Inc. Law, John (2017). “STS as Method”. In: The Handbook of Science and Technology Studies, edited by Ulrike Felt, Rayvon Fouché, Clark Miller, Laurel Smith–Doer. 4th ed. Cambridge–London: The MIT Press, pp. 31–57. Law, John; Singleton, Vicky (2015). “ANT, Multiplicity and Policy”. heterogenities.net. Available at http://www.heterogeneities.net/publications/LawSingleton2014ANTMultiplicityPolicy.pdf. Last Accessed on 27/01/2018. Mol, Annemarie (1999). “Ontological Politics. A Word and Some Questions”. The Sociological Review 47, no. 1: pp. 74–89. doi: 10.1111/j.1467-954X.1999.tb03483.x Mol, Annemarie (2002). The Body Multiple. Ontology in Medical Practice. Durham–London: Duke University Press. doi: 10.1215/9780822384151 Natale, Simone; Ballatore, Andrea (2017). “Imagining the thinking machine. Technological myths and the rise of artificial intelligence”. The International Journal of Research into New Media Technologies 20, no. 10: pp. 1–16. P7000 Working Group (2018). Meeting minute. Draft Agenda. With assistance of Sarah Spiekermann, Noah Brodbeck, Ali Hessami, Annette Reilly, Sara Jordan, Vicky Hailey. IEEE. Available online at http://sites.ieee.org/sagroups-7000/files/2018/03/20180126-P7000-Draft-Minutes.pdf. Last Accessed on 27/01/2018. Puig de la Bellacasa, Maria (2015). “Making time for soil. Technoscientific futurity and the pace of care”. Social Studies of Science 45, no. 5: pp. 691–716. doi: 10.1177/0306312715599851 Rogers, Richard; Sánchez, Natalia; Kil, Aleksandra (2015). Issue Mapping for an Ageing Europe. Amsterdam: Amsterdam University Press. Shankar, Kalpana; Hakken, David; Østerlund, Carsten (2017). “Rethinking Documents”. In: The Handbook of 
PRACTICES OF GOVERNING AND MAKING ARTIFICIAL INTELLIGENCES  |  38   
Disputatio (2018) 8: a010  Science and Technology Studies, edited by Ulrike Felt, Rayvon Fouché, Clark Miller, Laurel Smith–Doer. 4th ed. Cambridge–London: The MIT Press, pp. 59–85. Star, Susan Leigh (1999). “The Ethnography of Infrastructure”. American Behavioral Scientist 43, no. 3: pp. 377–391. doi: 10.1177/00027649921955326 Stengers, Isabelle (2005). “The Cosmopolitical Proposal”. In: Making Things Public. Atmospheres of Democracy, edited by Bruno Latour, Peter Weibel. Cambridge–Karslruhe: MIT and ZKM, pp. 994–1003. Stola, Stephen; Bowker, Geoffrey (2017). “How Infrastructures Matter”. In: The Handbook of Science and Technology Studies, edited by Ulrike Felt, Rayvon Fouché, Clark Miller, Laurel Smith–Doer. 4th ed. Cambridge–London: The MIT Press, pp. 529–554. Timmermans, Stefan; Epstein, Steven (2010). “A World of Standards but not a Standard World. Toward a Sociology of Standards and Standardization”. Annual Review of Sociology 36, no. 1: pp. 69–89. doi: 10.1146/annurev.soc.012809.102629 Venturini, Tommaso (2010a). “Building on faults. How to represent controversies with digital methods”. Public Understanding of Science 21, no. 7: pp. 796–812. doi: 10.1177/0963662510387558 Venturini, Tommaso (2010b). “Diving in magma. How to explore controversies with actor–network theory”. Public Understanding of Science 19, no. 3: pp. 258–273. doi: 10.1177/0963662509102694 Venturini, Tommaso; Bounegru, Liliana; Jacomy, Mathieu; Gray, Jonathan (2017). “How to Tell Stories with Networks. Exploring the Narrative Affordances of Graphs with the Iliad”. In: The Datafied Society. Studying Culture through Data, edited by Mirko Tobias Schäfer and Karin van Es. Amsterdam: Amsterdam University Press, pp. 155–170. doi: 10.1515/9789048531011-014 Winner, Langdon (1980). “Do Artifacts Have Politics?” Daedalus 109, no. 1: pp. 121–136. Zarsky, Tal (2016). “The Trouble with Algorithmic Decisions. An Analytic Road Map to Examine Efficiency and Fairness in Automated and Opaque Decision Making”. Science, Technology, & Human Values 41, no. 1: pp. 118–132. doi: 10.1177/0162243915605575     
